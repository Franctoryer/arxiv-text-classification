{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DreamDreamer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "å”¯ä¸€çš„æ ‡ç­¾å€¼: ['cs_AI', 'cs_CE', 'cs_DS', 'cs_IT', 'cs_NE', 'cs_PL', 'cs_SY', 'cs_cv', 'math_AC', 'math_GR', 'math_ST']\n",
      "DatasetDict({\n",
      "    data: Dataset({\n",
      "        features: ['review_id', 'label', 'keywords', 'keysentences', 'abstract'],\n",
      "        num_rows: 550\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, ClassLabel\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report, accuracy_score\n",
    "\n",
    "# æ£€æŸ¥è®¾å¤‡\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ä» CSV åŠ è½½æ•°æ®\n",
    "data_files = {\"data\": \"../datasets/datasets.csv\"}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "# å°†æ ‡ç­¾ä¸­çš„ . æ›¿æ¢ä¸º _\n",
    "def replace_dot_with_underscore(example):\n",
    "    return {\"label\": example[\"label\"].replace(\".\", \"_\")}\n",
    "dataset = dataset.map(replace_dot_with_underscore)\n",
    "\n",
    "# è·å–å”¯ä¸€çš„æ ‡ç­¾å€¼\n",
    "unique_labels = sorted(set(dataset[\"data\"][\"label\"]))\n",
    "print(\"å”¯ä¸€çš„æ ‡ç­¾å€¼:\", unique_labels)\n",
    "\n",
    "# å°† label åˆ—è½¬æ¢ä¸º ClassLabel ç±»å‹\n",
    "dataset = dataset.cast_column(\"label\", ClassLabel(names=unique_labels))\n",
    "\n",
    "# æ‰“å°æ•°æ®é›†ä¿¡æ¯\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# å°† keywordsã€keysentences å’Œ abstract æ‹¼æ¥æˆ text\n",
    "def concatenate_text(example):\n",
    "    text = ' '.join([str(example['keywords']), str(example['keysentences']), str(example['abstract'])])\n",
    "    return {'text': text}\n",
    "dataset = dataset.map(concatenate_text)\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œ80%è®­ç»ƒï¼Œ20%éªŒè¯\n",
    "dataset = dataset['data'].train_test_split(test_size=0.2, stratify_by_column='label', seed=42)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n",
    "# è·å–åˆ†ç±»ä¸ªæ•°\n",
    "num_classes = len(train_dataset.features['label'].names)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# åŠ è½½é¢„è®­ç»ƒçš„ BERT æ¨¡å‹å’Œ tokenizer\n",
    "model_name = 'bert-base-uncased'  # æˆ–è€…é€‰æ‹©å…¶ä»–é€‚åˆçš„ BERT å˜ä½“ï¼Œå¦‚ 'bert-large-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†\n",
    "def encode_batch(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=512)  # BERT çš„æœ€å¤§åºåˆ—é•¿åº¦ä¸º 512\n",
    "\n",
    "encoded_train_dataset = train_dataset.map(encode_batch, batched=True, batch_size=32)\n",
    "encoded_eval_dataset = eval_dataset.map(encode_batch, batched=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DreamDreamer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\DreamDreamer\\AppData\\Local\\Temp\\ipykernel_31600\\3307635668.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# è®¾ç½®å‚æ•°ï¼Œç”¨ Trainer() åˆ›å»ºå®ä¾‹\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=16,  # æ ¹æ® GPU å†…å­˜è°ƒæ•´\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    save_strategy='epoch',  # ä¿å­˜ç­–ç•¥\n",
    "    load_best_model_at_end=True,  # åœ¨è®­ç»ƒç»“æŸæ—¶åŠ è½½æœ€å¥½çš„æ¨¡å‹\n",
    "    metric_for_best_model='macro_f1',  # ç”¨äºé€‰æ‹©æœ€å¥½çš„æ¨¡å‹çš„æŒ‡æ ‡\n",
    ")\n",
    "\n",
    "# å®šä¹‰ compute_metrics å‡½æ•°\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    macro_f1 = f1_score(labels, preds, average='macro')\n",
    "    return {\"accuracy\": acc, \"macro_f1\": macro_f1}\n",
    "\n",
    "# åˆ›å»º Trainer å®ä¾‹\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 20%|â–ˆâ–ˆ        | 28/140 [02:01<05:58,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4291977882385254, 'eval_accuracy': 0.07272727272727272, 'eval_macro_f1': 0.0313337456194599, 'eval_runtime': 3.1545, 'eval_samples_per_second': 34.87, 'eval_steps_per_second': 2.219, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 56/140 [04:08<04:37,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3909237384796143, 'eval_accuracy': 0.13636363636363635, 'eval_macro_f1': 0.07963052989790959, 'eval_runtime': 3.5444, 'eval_samples_per_second': 31.035, 'eval_steps_per_second': 1.975, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 84/140 [06:20<03:10,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.354708194732666, 'eval_accuracy': 0.12727272727272726, 'eval_macro_f1': 0.0779904398986178, 'eval_runtime': 3.7805, 'eval_samples_per_second': 29.097, 'eval_steps_per_second': 1.852, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 112/140 [08:33<01:35,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.364818811416626, 'eval_accuracy': 0.14545454545454545, 'eval_macro_f1': 0.09235332276214486, 'eval_runtime': 4.0226, 'eval_samples_per_second': 27.346, 'eval_steps_per_second': 1.74, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [10:48<00:00,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.240762948989868, 'eval_accuracy': 0.3, 'eval_macro_f1': 0.24660158304387503, 'eval_runtime': 2.3303, 'eval_samples_per_second': 47.205, 'eval_steps_per_second': 3.004, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [10:56<00:00,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 656.1073, 'train_samples_per_second': 3.353, 'train_steps_per_second': 0.213, 'train_loss': 2.358786228724888, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=140, training_loss=2.358786228724888, metrics={'train_runtime': 656.1073, 'train_samples_per_second': 3.353, 'train_steps_per_second': 0.213, 'total_flos': 578891096678400.0, 'train_loss': 2.358786228724888, 'epoch': 5.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è®­ç»ƒæ¨¡å‹\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 2.240762948989868, 'eval_accuracy': 0.3, 'eval_macro_f1': 0.24660158304387503, 'eval_runtime': 2.0777, 'eval_samples_per_second': 52.944, 'eval_steps_per_second': 3.369, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:02<00:00,  3.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# è¯„ä¼°æ¨¡å‹\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", results)\n",
    "\n",
    "# è·å–é¢„æµ‹ç»“æœ\n",
    "predictions = trainer.predict(encoded_eval_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "labels = predictions.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# è®¡ç®—æ··æ·†çŸ©é˜µ\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m(labels, preds)\n\u001b[0;32m      3\u001b[0m cm_normalized \u001b[38;5;241m=\u001b[39m confusion_matrix(labels, preds, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# è·å–æ ‡ç­¾åç§°\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# è®¡ç®—æ··æ·†çŸ©é˜µ\n",
    "cm = confusion_matrix(labels, preds)\n",
    "cm_normalized = confusion_matrix(labels, preds, normalize='true')\n",
    "\n",
    "# è·å–æ ‡ç­¾åç§°\n",
    "label_names = unique_labels\n",
    "\n",
    "# ç»˜åˆ¶æ··æ·†çŸ©é˜µ\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", xticklabels=label_names, yticklabels=label_names, cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('BERT Confusion Matrix')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results/(test)BERT Confusion Matrix.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 Score: 0.2466\n",
      "\n",
      "Classification Report:\n",
      "{'cs_AI': {'precision': 0.2727272727272727, 'recall': 0.3, 'f1-score': 0.2857142857142857, 'support': 10.0}, 'cs_CE': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 10.0}, 'cs_DS': {'precision': 0.5, 'recall': 0.2, 'f1-score': 0.2857142857142857, 'support': 10.0}, 'cs_IT': {'precision': 0.1891891891891892, 'recall': 0.7, 'f1-score': 0.2978723404255319, 'support': 10.0}, 'cs_NE': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 10.0}, 'cs_PL': {'precision': 0.4, 'recall': 0.2, 'f1-score': 0.26666666666666666, 'support': 10.0}, 'cs_SY': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 10.0}, 'cs_cv': {'precision': 0.35294117647058826, 'recall': 0.6, 'f1-score': 0.4444444444444444, 'support': 10.0}, 'math_AC': {'precision': 0.5625, 'recall': 0.9, 'f1-score': 0.6923076923076923, 'support': 10.0}, 'math_GR': {'precision': 0.42857142857142855, 'recall': 0.3, 'f1-score': 0.35294117647058826, 'support': 10.0}, 'math_ST': {'precision': 0.07692307692307693, 'recall': 0.1, 'f1-score': 0.08695652173913043, 'support': 10.0}, 'accuracy': 0.3, 'macro avg': {'precision': 0.2529865585346869, 'recall': 0.3, 'f1-score': 0.24660158304387503, 'support': 110.0}, 'weighted avg': {'precision': 0.2529865585346869, 'recall': 0.3, 'f1-score': 0.24660158304387503, 'support': 110.0}}\n",
      "Classification report has been saved to ./results/(test)BERT_classification_report.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DreamDreamer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\DreamDreamer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\DreamDreamer\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\DreamDreamer\\AppData\\Local\\Temp\\ipykernel_31600\\2104711395.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_report.loc['macro_f1'] = {'precision': None, 'recall': None, 'f1-score': macro_f1, 'support': None}\n"
     ]
    }
   ],
   "source": [
    "# è®¡ç®—å¹¶æ‰“å° Macro F1\n",
    "macro_f1 = f1_score(labels, preds, average='macro')\n",
    "print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "\n",
    "report = classification_report(labels, preds, target_names=label_names, output_dict=True)  # ä½¿ç”¨ output_dict=True ç”Ÿæˆå­—å…¸\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "# è½¬æ¢åˆ†ç±»æŠ¥å‘Šä¸º DataFrame\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "\n",
    "# æ·»åŠ  Macro F1 åˆ°åˆ†ç±»æŠ¥å‘Š\n",
    "df_report.loc['macro_f1'] = {'precision': None, 'recall': None, 'f1-score': macro_f1, 'support': None}\n",
    "\n",
    "# æŒ‡å®š CSV æ–‡ä»¶è·¯å¾„\n",
    "output_csv_path = './results/(test)BERT_classification_report.csv'\n",
    "\n",
    "# ä¿å­˜ä¸º CSV æ–‡ä»¶\n",
    "df_report.to_csv(output_csv_path, index=True)\n",
    "print(f\"Classification report has been saved to {output_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
